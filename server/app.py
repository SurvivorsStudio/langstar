

from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel
from typing import Dict, Any
import datetime

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # or specify frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

class PromptNodeInput(BaseModel):
    prompt: str
    param: Dict[str, Any]
    return_key: str
    

@app.post('/workflow/node/promptnode')
def prompt_node(data: PromptNodeInput):
    print( data ) 
    template = PromptTemplate(
        template=data.prompt,
        input_variables=list(data.param.keys())
    )
    rendered = template.format(**data.param)
    data.param[data.return_key] = rendered
    print( data.param )
    return data.param
    

@app.post('/workflow/node/startnode')
def start_node(msg: dict = Body(...)):  # msgë¥¼ dictë¡œ ë°›ìŒ
    print(msg) 
    result = {}
    for row in msg["variables"] : 
        result[row['variableName']] = row['defaultValue']
    
    return result


##########################################################

from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_aws import ChatBedrockConverse

from langchain.tools import Tool
import types

from typing import Dict, Tuple
from datetime import datetime, timedelta
from typing import List, Optional
from langchain.memory import ConversationBufferMemory


### stringì„ íˆ´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ 
def create_tool_from_api(tool_name: str, tool_description: str, tool_code: str) -> Tool:
    """
    ë¬¸ìì—´ë¡œ ì „ë‹¬ëœ Python í•¨ìˆ˜ ì½”ë“œì™€ ì´ë¦„, ì„¤ëª…ìœ¼ë¡œ LangChain Tool ê°ì²´ë¥¼ ìƒì„±
    """

    # ì•ˆì „í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
    tool_namespace = {}

    try:
        # ì „ë‹¬ë°›ì€ ì½”ë“œ ì‹¤í–‰
        exec(tool_code, tool_namespace)
    except Exception as e:
        raise ValueError(f"ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # í•¨ìˆ˜ ê°ì²´ ì¶”ì¶œ (ê°€ì •: í•¨ìˆ˜ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬)
    tool_func = None
    for v in tool_namespace.values():
        if isinstance(v, types.FunctionType):
            tool_func = v
            break

    if not tool_func:
        raise ValueError("ì œê³µëœ ì½”ë“œì—ì„œ í•¨ìˆ˜ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Tool ê°ì²´ ìƒì„±
    return Tool.from_function(
        name=tool_name,
        description=tool_description,
        func=tool_func
    )



# ì…ë ¥ ëª¨ë¸ ì •ì˜
class AgentNodeInput(BaseModel):
    input: str
    return_key: str


MEMORY_STORE = {}

# ì‹¤ì œ í˜¸ì¶œ ì—”ë“œí¬ì¸íŠ¸
@app.post('/workflow/node/agentnode')
def agent_node(msg: dict = Body(...)):  # msgë¥¼ dictë¡œ ë°›ìŒ
    print( "node start : ", datetime.datetime.now() ) 
    print( msg )

    
    model_id = msg['model'] 
    system_prompt =  msg['system_prompt'] 
    user_prompt = msg['user_prompt'] 
    return_key = msg['return_key']
    tools_data = msg['tools']
    tools = [create_tool_from_api(**tool_info) for tool_info in tools_data]

    if len(system_prompt) == 0 : 
        system_prompt = "ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤" 

    # ëª¨ë¸ ì„¤ì •
    llm = ChatBedrockConverse(
        model="us.amazon.nova-pro-v1:0", 
        temperature=0.1,
        max_tokens=1000
    )

    # ë©”ëª¨ë¦¬ê°€ ìˆëŠ” ê²½ìš° 
    if len(memory_type) > 0 : 
        memory_group_name = msg['memory_group_name']
        chat_id           = msg['chat_id']
        memory_type       = msg['memory_type']
        
        if memory_type == "ConversationBufferMemory" : 
            if (chat_id not in MEMORY_STORE) : 
                memory = ConversationBufferMemory(memory_key=memory_group_name, return_messages=True)
                MEMORY_STORE[chat_id] = { memory_group_name : memory } 
            elif (chat_id in MEMORY_STORE) : 
                if memory_group_name not in MEMORY_STORE[chat_id] : 
                    MEMORY_STORE[chat_id][memory_group_name] = memory 
                    

        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""{system_prompt}
            ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            """),
            MessagesPlaceholder(variable_name=memory_group_name),
            ("human", "{input}"),            
            MessagesPlaceholder(variable_name="agent_scratchpad")  # í•„ìˆ˜ ë³€ìˆ˜
        ])
        
        agent = create_openai_tools_agent(llm, tools, MEMORY_STORE[chat_id][memory_group_name], prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
        result = agent_executor.invoke({"input": user_prompt})

        

    # ë©”ëª¨ë¦¬ê°€ ì—†ëŠ” ê²½ìš° 
    else : 
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"""{system_prompt}
            ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”.
            """),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")  # í•„ìˆ˜ ë³€ìˆ˜
        ])

        agent = create_openai_tools_agent(llm, tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
        result = agent_executor.invoke({"input": user_prompt})

    print( "node end : ", datetime.datetime.now() ) 
        
        
    return result['output'][0]['text']





# ğŸ“Œ ë©”ëª¨ë¦¬ ì €ì¥ì†Œì™€ TTL
# user_memories: Dict[str, Tuple[AgentExecutor, datetime]] = {}
# MEMORY_TTL = timedelta(minutes=30)

# # ğŸ“Œ ìš”ì²­ ìŠ¤í‚¤ë§ˆ
# class ChatRequest(BaseModel):
#     model: str
#     system_prompt: str
#     user_prompt: str
#     memory_group: str
#     memory_group_name: str
#     tools: Optional[List[dict]] = []  # toolsê°€ ë¹ˆ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ Optionalë¡œ ì²˜ë¦¬
#     memory_type: str
#     return_key: str
#     chat_id: Optional[str] = None  # ìš”ì²­ì— ì—†ì„ ìˆ˜ë„ ìˆìœ¼ë¯€ë¡œ Optional ì²˜ë¦¬

# class ResetRequest(BaseModel):
#     memory_group: str
#     chat_id: str

# # ğŸ“Œ ì˜¤ë˜ëœ ë©”ëª¨ë¦¬ ì •ë¦¬ í•¨ìˆ˜
# def clean_expired_memories():
#     now = datetime.utcnow()
#     expired = [k for k, (_, ts) in user_memories.items() if now - ts > MEMORY_TTL]
#     for k in expired:
#         del user_memories[k]

# # ğŸ“Œ 1ë¶„ë§ˆë‹¤ ì‹¤í–‰ë˜ëŠ” ìë™ ì •ë¦¬ ì‘ì—…
# # @app.on_event("startup")
# # @repeat_every(seconds=60)
# def remove_stale_memories_task():
#     clean_expired_memories()

# # ğŸ“Œ ì—ì´ì „íŠ¸ ìƒì„± í•¨ìˆ˜
# def create_agent_executor(tools, format_instructions, memory_group_name):
#     memory = ConversationBufferMemory(
#         memory_key="chat_history",
#         return_messages=True
#     )

#     #     if system_prompt is not None:
# #         prompt = ChatPromptTemplate.from_messages([
# #             ("system", f"""{system_prompt}
# #             ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”.
# #             """),
# #             ("human", "{input}"),
# #             MessagesPlaceholder(variable_name="agent_scratchpad")  # í•„ìˆ˜ ë³€ìˆ˜
# #         ])

#     prompt = ChatPromptTemplate.from_messages([
#         ("system", f"""
#         ë‹¹ì‹ ì€ ì§ˆë¬¸ì— ë‹µë³€í•˜ëŠ” ë„ìš°ë¯¸ì…ë‹ˆë‹¤.

#         ì‚¬ìš© ê°€ëŠ¥í•œ ë„êµ¬:
#         {{tools}}


#         ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”.
#         """),
#         MessagesPlaceholder(variable_name="chat_history"),
#         ("human", "{input}"),
#         MessagesPlaceholder(variable_name="agent_scratchpad")
#     ])

#     llm = ChatBedrockConverse(
#         model="us.amazon.nova-pro-v1:0",
#         temperature=0.1,
#         max_tokens=1000,
#     )

#     agent = create_openai_tools_agent(
#         llm,
#         tools,
#         prompt.partial(
#             format_instructions=format_instructions,
#             tools="\n".join([f"- {tool['name']}: {tool['description']}" for tool in tools]) if tools else "ì—†ìŒ"
#         )
#     )

#     return AgentExecutor(
#         agent=agent,
#         tools=tools,
#         memory=memory,
#         verbose=True
#     )

# # âœ… ì—ì´ì „íŠ¸ ìš”ì²­ API ê²½ë¡œ ë³€ê²½ë¨
# @app.post("/workflow/node/agentnode")
# async def agent_node_endpoint(request: ChatRequest):
#     # clean_expired_memories()

#     print( request ) 
#     key = f"{request.memory_group}|{request.chat_id}"

#     if key not in user_memories:
#         executor = create_agent_executor(request.tools, '{"answer": "<ì—¬ê¸°ì— ë‹µë³€ì„ ì‘ì„±í•˜ì„¸ìš”>"}', request.memory_group_name)
#         user_memories[key] = (executor, datetime.utcnow())
#     else:
#         executor, _ = user_memories[key]
#         user_memories[key] = (executor, datetime.utcnow())  # ì—…ë°ì´íŠ¸

#     result = await executor.ainvoke({"input": request.user_prompt})
#     print( "ìˆ˜ì • í•„ìš” : ", result ) 
#     return result['output'][0]['text']
# import datetime
# from fastapi.responses import JSONResponse
# # ë©”ëª¨ë¦¬ ì €ì¥ì†Œ: {memory_group: {"memory": ..., "last_used": datetime}}
# MEMORY_STORE: Dict[str, Dict] = {}

# # âœ… ì‹¤ì œ í˜¸ì¶œ ì—”ë“œí¬ì¸íŠ¸

# # âœ… ì‹¤ì œ í˜¸ì¶œ ì—”ë“œí¬ì¸íŠ¸
# @app.post('/workflow/node/agentnode')
# def agent_node(msg: dict = Body(...)):
#     print("ğŸ“¥ Node start:", datetime.datetime.now())
#     print("Payload:", msg)

#     # ìš”ì²­ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
#     chat_id = msg['chat_id']
#     memory_group = msg['memory_group']
#     memory_group_name = msg['memory_group_name']
#     model_id = msg['model']
#     system_prompt = msg['system_prompt']
#     user_prompt = msg['user_prompt']
#     return_key = msg['return_key']
#     tools_data = msg.get('tools', [])
#     memory_type = msg.get('memory_type', 'ConversationBufferMemory')

#     # ë©”ëª¨ë¦¬ ì¡°íšŒ ë˜ëŠ” ìƒì„±
#     if chat_id not in MEMORY_STORE:
#         MEMORY_STORE[chat_id] = {}

#     if memory_group_name not in MEMORY_STORE[chat_id]:
#         if memory_type == "ConversationBufferMemory":
#             memory = ConversationBufferMemory(
#                 return_messages=True,
#                 memory_key="chat_history"
#             )
#         else:
#             return JSONResponse(status_code=400, content={"error": "ì§€ì›í•˜ì§€ ì•ŠëŠ” memory_type"})

#         MEMORY_STORE[chat_id][memory_group_name] = {
#             "memory": memory,
#             "last_used": datetime.datetime.now(),
#             "group": memory_group,
#         }
#     else:
#         memory = MEMORY_STORE[chat_id][memory_group_name]["memory"]
#         MEMORY_STORE[chat_id][memory_group_name]["last_used"] = datetime.datetime.now()

#     # ë„êµ¬ ì¤€ë¹„
#     tools = [create_tool_from_api(**tool_info) for tool_info in tools_data]

#     # ëª¨ë¸ ì„¤ì •
#     llm = ChatBedrockConverse(
#         model="us.amazon.nova-pro-v1:0",
#         temperature=0.1,
#         max_tokens=1000
#     )

#     # í”„ë¡¬í”„íŠ¸ ì •ì˜
#     prompt = ChatPromptTemplate.from_messages([
#         ("system", f"""{system_prompt}
# ì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”."""),
#         MessagesPlaceholder(variable_name="chat_history"),
#         ("human", "{input}"),
#         MessagesPlaceholder(variable_name="agent_scratchpad")
#     ])

#     # ì—ì´ì „íŠ¸ êµ¬ì„±
#     agent = create_openai_tools_agent(llm, tools, prompt)
#     agent_executor = AgentExecutor(agent=agent, tools=tools, memory=memory, verbose=True)

#     # ì‹¤í–‰
#     try:
#         result = agent_executor.invoke({"input": user_prompt})
#         raw_output = result.get(return_key)
#         parsed_output = json.loads(raw_output) if isinstance(raw_output, str) else {"answer": raw_output}
#     except Exception as e:
#         return JSONResponse(status_code=500, content={"error": str(e)})

#     print("âœ… Node end:", datetime.datetime.now())
    
#     print(  MEMORY_STORE[chat_id][memory_group_name] ) 

#     # ì‘ë‹µ í¬ë§· ë³€ê²½
#     return result['output'][0]['text']
