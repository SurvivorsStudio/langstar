from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from langchain_core.prompts import PromptTemplate
from pydantic import BaseModel
from typing import Dict, Any
import datetime
import uuid
import ast
import signal
import sys
import os
from contextlib import asynccontextmanager
import logging

# ì»¤ìŠ¤í…€ ë¡œê±° ì„¤ì •
class ColoredFormatter(logging.Formatter):
    """ì—ëŸ¬ ì‹œ ìƒ‰ìƒì„ ë³€ê²½í•˜ëŠ” ì»¤ìŠ¤í…€ í¬ë§¤í„°"""
    
    COLORS = {
        'ERROR': '\033[91m',    # ë¹¨ê°„ìƒ‰
        'WARNING': '\033[93m',  # ë…¸ë€ìƒ‰
        'INFO': '\033[92m',     # ì´ˆë¡ìƒ‰
        'DEBUG': '\033[94m',    # íŒŒë€ìƒ‰
        'RESET': '\033[0m'      # ë¦¬ì…‹
    }
    
    def format(self, record):
        # ì—ëŸ¬ ë ˆë²¨ì— ë”°ë¼ ìƒ‰ìƒ ì ìš©
        color = self.COLORS.get(record.levelname, self.COLORS['RESET'])
        record.levelname = f"{color}{record.levelname}{self.COLORS['RESET']}"
        return super().format(record)

# ë¡œê±° ì„¤ì •
logger = logging.getLogger("uvicorn")
handler = logging.StreamHandler()
handler.setFormatter(ColoredFormatter('%(levelname)s: %(message)s'))
logger.addHandler(handler)
logger.setLevel(logging.INFO)

app = FastAPI()

# ì•ˆì „í•œ ì¢…ë£Œë¥¼ ìœ„í•œ ì‹œê·¸ë„ í•¸ë“¤ëŸ¬
def signal_handler(signum, frame):
    print("\n" + "="*50)
    print("ğŸ›‘ Shutting down server safely...")
    print("="*50)
    # sys.exit(0) ëŒ€ì‹  os._exit(0) ì‚¬ìš©í•˜ì—¬ ê¹”ë”í•˜ê²Œ ì¢…ë£Œ
    os._exit(0)

# SIGINT (Ctrl+C)ì™€ SIGTERM ì‹œê·¸ë„ ë“±ë¡
signal.signal(signal.SIGINT, signal_handler)
signal.signal(signal.SIGTERM, signal_handler)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # or specify frontend URL
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ì•± ì‹œì‘ ì‹œ ì¦‰ì‹œ ì‹¤í–‰ë˜ëŠ” ë©”ì‹œì§€
sys.stdout.write("\n" + "="*60 + "\n")
sys.stdout.write("ğŸš€ LangStar server has started!\n")
sys.stdout.write("="*60 + "\n\n")
sys.stdout.flush()

class PromptNodeInput(BaseModel):
    prompt: str
    param: Dict[str, Any]
    return_key: str
    

@app.post('/workflow/node/promptnode')
def prompt_node(data: PromptNodeInput):
    print( data ) 
    template = PromptTemplate(
        template=data.prompt,
        input_variables=list(data.param.keys())
    )
    rendered = template.format(**data.param)
    data.param[data.return_key] = rendered
    print( data.param )
    return data.param

@app.post('/workflow/node/pythonnode')
def prompt_node(msg: dict = Body(...)): 
    python_code = msg['py_code'] 
    param = msg.get("param", {})
    
    parsed = ast.parse(python_code)
    func_def = next((node for node in parsed.body if isinstance(node, ast.FunctionDef)), None)
    
    if not func_def:
        return {"error": "No function found in python_code."}

    # í•¨ìˆ˜ ì´ë¦„ 
    function_name = func_def.name
    # ì¸í’‹ ë¦¬ìŠ¤íŠ¸ ( í–¥í›„ ì‚¬ìš© ) 
    # param_names = [arg.arg for arg in func_def.args.args]

    exec_globals = {}
    
    try:
        # ì‚¬ìš©ì ì½”ë“œë¥¼ ì‹¤í–‰í•˜ì—¬ í•¨ìˆ˜ ì •ì˜
        exec(python_code, exec_globals)
        func = exec_globals[function_name]
       
        result = func(param)
        return result
    
    except Exception as e:
        return {"error": str(e)}
    

@app.post('/workflow/node/startnode')
def start_node(msg: dict = Body(...)):  # msgë¥¼ dictë¡œ ë°›ìŒ
    print(msg) 
    result = {}
    for row in msg["variables"] : 
        result[row['variableName']] = row['defaultValue']
    
    return result


##########################################################

from fastapi import FastAPI, Body
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
from typing import Dict, Any

from langchain.agents import AgentExecutor, create_openai_tools_agent
from langchain.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_aws import ChatBedrockConverse

from langchain.tools import Tool
import types

from typing import Dict, Tuple
from datetime import datetime, timedelta
from typing import List, Optional
from langchain.memory import ConversationBufferMemory


### stringì„ íˆ´ë¡œ ë³€í™˜í•˜ëŠ” í•¨ìˆ˜ 
def create_tool_from_api(tool_name: str, tool_description: str, tool_code: str) -> Tool:
    """
    ë¬¸ìì—´ë¡œ ì „ë‹¬ëœ Python í•¨ìˆ˜ ì½”ë“œì™€ ì´ë¦„, ì„¤ëª…ìœ¼ë¡œ LangChain Tool ê°ì²´ë¥¼ ìƒì„±
    """

    # ì•ˆì „í•œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ìƒì„±
    tool_namespace = {}

    try:
        # ì „ë‹¬ë°›ì€ ì½”ë“œ ì‹¤í–‰
        exec(tool_code, tool_namespace)
    except Exception as e:
        raise ValueError(f"ì½”ë“œ ì‹¤í–‰ ì‹¤íŒ¨: {e}")

    # í•¨ìˆ˜ ê°ì²´ ì¶”ì¶œ (ê°€ì •: í•¨ìˆ˜ê°€ í•˜ë‚˜ë§Œ ì¡´ì¬)
    tool_func = None
    for v in tool_namespace.values():
        if isinstance(v, types.FunctionType):
            tool_func = v
            break

    if not tool_func:
        raise ValueError("ì œê³µëœ ì½”ë“œì—ì„œ í•¨ìˆ˜ ê°ì²´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

    # Tool ê°ì²´ ìƒì„±
    return Tool.from_function(
        name=tool_name,
        description=tool_description,
        func=tool_func
    )



# ì…ë ¥ ëª¨ë¸ ì •ì˜
class AgentNodeInput(BaseModel):
    input: str
    return_key: str


MEMORY_STORE = {}

@app.post('/workflow/node/agentnode')
def agent_node(msg: dict = Body(...)):
    print(msg)
    model_id = msg['model']
    system_prompt = msg.get('system_prompt', "ë‹¹ì‹ ì€ AI ë„ìš°ë¯¸ì…ë‹ˆë‹¤")
    user_prompt = msg['user_prompt']
    return_key = msg['return_key']
    tools_data = msg['tools']
    tools = [create_tool_from_api(**tool_info) for tool_info in tools_data]
    memory_type = msg.get('memory_type', "")
    
    llm = ChatBedrockConverse(
        model="us.amazon.nova-pro-v1:0",
        temperature=0.1,
        max_tokens=1000
    )
    
    if memory_type:
        memory_group_name = msg['memory_group_name']
        chat_id = msg.get('chat_id', str(uuid.uuid1()))
        
        # Initialize memory if not exists
        if memory_type == "ConversationBufferMemory":
            if chat_id not in MEMORY_STORE:
                memory = ConversationBufferMemory(memory_key=memory_group_name, return_messages=True)
                MEMORY_STORE[chat_id] = {memory_group_name: memory}
            elif memory_group_name not in MEMORY_STORE[chat_id]:
                memory = ConversationBufferMemory(memory_key=memory_group_name, return_messages=True)
                MEMORY_STORE[chat_id][memory_group_name] = memory
        
        memory = MEMORY_STORE[chat_id][memory_group_name]
        
        # Create prompt with memory placeholder
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"{system_prompt}\nì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
            MessagesPlaceholder(variable_name=memory_group_name),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
        
        # Create agent with prompt only (no memory parameter)
        agent = create_openai_tools_agent(llm, tools, prompt)
        
        # Create agent executor with memory
        agent_executor = AgentExecutor(
            agent=agent, 
            tools=tools, 
            memory=memory,  # Memory goes here in AgentExecutor
            verbose=True
        )
        
        try:
            result = agent_executor.invoke({"input": user_prompt})
            return  result['output'][0]['text']
        except Exception as e:
            return {"error": str(e)}
    
    else:
        # No memory case
        prompt = ChatPromptTemplate.from_messages([
            ("system", f"{system_prompt}\nì´ì „ ëŒ€í™” ë‚´ìš©ì„ ì°¸ê³ í•˜ì—¬ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ë§¥ë½ì— ë§ê²Œ ë‹µë³€í•˜ì„¸ìš”."),
            ("human", "{input}"),
            MessagesPlaceholder(variable_name="agent_scratchpad")
        ])
        
        # Create agent without memory
        agent = create_openai_tools_agent(llm, tools, prompt)
        agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)
        
        try:
            result = agent_executor.invoke({"input": user_prompt})
            return result['output'][0]['text']
        except Exception as e:
            return {"error": str(e)}



#####################################
import textwrap
import ast 

def create_state(config_json) : 
    code_lines = [
        "from pydantic import BaseModel",
        "from typing import Annotated",
        "import operator",
        "from langchain_core.prompts import PromptTemplate",
        "from langgraph.graph import StateGraph, START, END",
        "",
        "class MyState(BaseModel):",
        "    response:dict = {}"
    ]

    
    for config_token in config_json : 
        
        for config_key, config_val in config_token.items() : 
            # print( config_val ) 
            if '__annotated__' in config_val :
                # print( config_val ) 
                code_token = "    {0} :Annotated[dict, operator.or_] = ".format( config_key ) + "{}"
            else : 
                code_token = "    {0} :dict = {1}".format( config_key, str(config_val ) ) 
            
            code_lines.append( code_token ) 
    return "\n".join( code_lines ) 



def create_function_node(function_name, py_code, func_exec):
    # í•¨ìˆ˜ ë‚´ë¶€ìš© ì½”ë“œ ë¸”ë¡ì„ ì˜¬ë°”ë¥´ê²Œ ë“¤ì—¬ì“°ê¸° ì²˜ë¦¬
    indented_code = textwrap.indent(py_code, "    ")

    # ì „ì²´ í•¨ìˆ˜ ì½”ë“œ êµ¬ì„±
    function_node_code = f"""def node_{function_name}(state):
    my_name = "{function_name}"
    node_name = my_name
    node_config = my_name + "_Config"

    state_dict  = state.model_dump()

{indented_code}

    # í•¨ìˆ˜ ì‹¤í–‰
    input_param = state_dict[node_name ] 
    result = {func_exec}( input_param ) 
    
    node_config = state_dict[node_config]
    next_node_list = node_config.get('next_node', []) 
    return return_next_node( node_name, next_node_list, result ) 
    
"""
    return function_node_code.strip()
        
    

def react_to_python_langgraph(create_node_json) : 
    # node id to name 
    node_id_to_node_label = {}
    for node in create_node_json['nodes'] : 
        node_id    = node['id']
        node_type  = node['type']
        node_label = node['data']['label']
        
        node_id_to_node_label[node_id] = { 'node_name' : node_label, 'node_type' : node_type } 
    
    
    # node relation 
    edge_relation = {}
    for edge in create_node_json['edges'] : 
        source = edge['source'] 
        target = edge['target'] 
    
        source_node_name = node_id_to_node_label[source]['node_name']
        target_node_name = node_id_to_node_label[target]['node_name']
        target_node_type = node_id_to_node_label[target]['node_type']
        
        if source_node_name not in edge_relation: 
            edge_relation[source_node_name]= [ { 'node_name' :  target_node_name, 'node_type' : target_node_type }  ]
        else : 
            edge_relation[source_node_name].append( { 'node_name' :  target_node_name, 'node_type' : target_node_type   } ) 
    
    
    # create config 
    result = []
    for node in create_node_json['nodes'] : 
        if node['type'] == 'startNode': 
            node_id   = node['id']
            node_type = node['type']
            node_name = node['data']['label']
            node_config = node['data']['config']['variables']
            
            temp_config = {} 
            
            config_dict = {} 
            for token_config in node_config : 
                config_dict.update( { token_config['name'] : token_config['defaultValue'] } )
    
            # ë°ì´í„° ì—®ì–´ì£¼ê¸° 
            config_id = node_name + "_Config"
            temp_config[config_id]              = { 'config' : config_dict } 
            temp_config[config_id]['node_type'] = node_type
            temp_config[config_id]['next_node'] = edge_relation[node_name]
            temp_config[config_id]['node_name'] = node_name
    
            
            result.append( temp_config.copy() )
            result.append( {node_name : {}} ) 
            
        elif node['type'] == 'promptNode': 
            node_id   = node['id']
            node_type = node['type']
            node_name = node['data']['label']
            template = node['data']['config']['template']
            outputVariable = node['data']['config']['outputVariable']
    
            temp_config = {}
            config_id = node_name + "_Config"
            temp_config[config_id]              = { 'config' : {'template' : template} } 
            temp_config[config_id]['outputVariable'] = outputVariable
            temp_config[config_id]['node_type']      = node_type
            temp_config[config_id]['next_node']      = edge_relation[node_name]
            temp_config[config_id]['node_name'] = node_name
    
            
            result.append( temp_config.copy() )
            result.append( {node_name : {}} ) 
            
        elif node['type'] == 'mergeNode': 
            node_id   = node['id']
            node_type = node['type']
            node_name = node['data']['label']
            
            mergeMappings = node['data']['config']['mergeMappings']

            config_dict = {} 
            for return_style in mergeMappings : 
                output_value      = return_style['outputKey'] 
                source_node       = return_style['sourceNodeId'] 
                source_node_value = return_style['sourceNodeKey'] 
                
                source_node_name = node_id_to_node_label[source_node]['node_name']
                config_dict[output_value] = { 'node_name' : source_node_name , 'node_value' : source_node_value } 


            temp_config = {}
            config_id = node_name + "_Config"
            temp_config[config_id]                   = { 'config' : config_dict } 
            temp_config[config_id]['node_type']      = node_type
            temp_config[config_id]['next_node']      = edge_relation[node_name]
            temp_config[config_id]['node_name'] = node_name

            result.append( temp_config.copy() )
            result.append( {node_name : {"__annotated__": True}} )
    
        elif node['type'] == 'endNode': 
            node_id   = node['id']
            node_type = node['type']
            node_name = node['data']['label']
            outputVariable = node['data']['config']
    
            temp_config = {}
            config_id = node_name + "_Config"
            temp_config[config_id] = { 'config' : { 'receiveKey' : [ outputVariable['receiveKey'] ] } }
    
            result.append( temp_config.copy() )
            result.append( {node_name : {}} )       

        elif node['type'] == 'functionNode': 
            node_id   = node['id']
            node_type = node['type']
            node_name = node['data']['label']
            python_conde = node['data']['code']
            
            temp_config = {}
            config_id = node_name + "_Config"
            temp_config[config_id]              = { 'config' : {'code' : python_conde} } 
            temp_config[config_id]['node_type'] = node_type
            temp_config[config_id]['next_node'] = edge_relation[node_name]
            temp_config[config_id]['node_name'] = node_name

            
            result.append( temp_config.copy() )
            result.append( {node_name : {}} ) 


    python_code = class_state = create_state(result)
    python_code += """
def return_next_node( my_node, next_node_list, return_value  ):

    updates = {}
    for next_node in next_node_list : 
        next_name = next_node['node_name']
        next_type = next_node['node_type']
        
        # merge node 
        if next_type == 'mergeNode':
            if next_name not in updates:
                updates[next_name] = {my_node : return_value}
            else : 
                updates[next_name][my_node] = return_value
                
        # ì¼ë°˜ ë…¸ë“œ 
        else : 
            if next_name not in updates:
                updates[next_name] = return_value
                
    return updates
"""  + "\n" + "\n" + "\n"
    
    start_node_code = """
def node_**startNode**(state):
    my_name = "**startNode**" 
    node_name = my_name
    node_config = my_name + "_Config"


    # ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ ë°›ì€ ê°’ê³¼ ì„¤ì •ê°’ì„ ê°€ì§€ê³  ì˜¨ë‹¤. 
    state_dict  = state.model_dump()
    node_input  = state_dict[my_name] 
    node_config = state_dict[node_config] 

    # ë‹¤ìŒ ë…¸ë“œì— ì „ë‹¬í•˜ëŠ” ê°’ 
    return_value = { **node_config['config'], **node_input } 
    
    # ì „ë‹¬í•˜ê³ ì í•˜ëŠ” íƒ€ê²Ÿ node ë¦¬ìŠ¤íŠ¸ 
    next_node_list = node_config.get('next_node', []) 

    
    return return_next_node( node_name, next_node_list, return_value ) 
"""  + "\n" + "\n" + "\n"
    
    prompt_node_code = """
def node_**promptNode**(state):
    my_name = "**promptNode**" 
    node_name = my_name
    node_config = my_name + "_Config"

    # ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ ë°›ì€ ê°’ê³¼ ì„¤ì •ê°’ì„ ê°€ì§€ê³  ì˜¨ë‹¤. 
    state_dict  = state.model_dump()
    node_input  = state_dict[my_name] 
    node_config = state_dict[node_config]
    
    # prompt ìƒì„± 
    template = PromptTemplate(
        template=node_config['config']['template'],
        input_variables=list(node_input.keys())
    )

    prompt = template.format(**node_input)
    output_value = node_config['outputVariable']
    

    # ë‹¤ìŒ ë…¸ë“œì— ì „ë‹¬í•˜ëŠ” ê°’ 
    return_value = node_input.copy() 
    return_value.update( { output_value : prompt } ) 

    # ì „ë‹¬í•˜ê³ ì í•˜ëŠ” íƒ€ê²Ÿ node ë¦¬ìŠ¤íŠ¸ 
    next_node_list = node_config.get('next_node', []) 
    
    return return_next_node( node_name, next_node_list, return_value ) 
"""  + "\n" + "\n" + "\n"
    
    merge_node_code = """
def node_**mergeNode**(state):
    my_name = "**mergeNode**" 
    node_name = my_name
    node_config = my_name + "_Config"

    # ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ ë°›ì€ ê°’ê³¼ ì„¤ì •ê°’ì„ ê°€ì§€ê³  ì˜¨ë‹¤. 
    state_dict  = state.model_dump()
    node_input  = state_dict[my_name] 
    node_config = state_dict[node_config]

                
    # ë‹¤ìŒ ë…¸ë“œì— ì „ë‹¬í•˜ëŠ” ê°’ 
    return_value = {} 
    for key, value  in node_config['config'].items():
        match_node  = value['node_name'] 
        match_value = value['node_value'] 
        return_value[key]  = node_input[match_node][match_value]


    # ì „ë‹¬í•˜ê³ ì í•˜ëŠ” íƒ€ê²Ÿ node ë¦¬ìŠ¤íŠ¸ 
    next_node_list = node_config.get('next_node', []) 

    return return_next_node( node_name, next_node_list, return_value )   
"""  + "\n" + "\n" + "\n"
    
    end_node_code = """
def node_**endNode**( state ) : 
    my_name = "**endNode**" 
    node_name = my_name
    node_config = my_name + "_Config"

    # ì‚¬ìš©ìì—ê²Œ ì „ë‹¬ ë°›ì€ ê°’ê³¼ ì„¤ì •ê°’ì„ ê°€ì§€ê³  ì˜¨ë‹¤. 
    state_dict  = state.model_dump()
    node_input  = state_dict[my_name] 
    node_config = state_dict[node_config]['config']['receiveKey']

    # ë‹¤ìŒ ë…¸ë“œì— ì „ë‹¬í•˜ëŠ” ê°’ 
    return_value = {} 
    if len(node_config) == 1 and node_config[0] == '': 
        return_value = node_input
    else : 
        for key in node_config: 
            return_value[key] = node_input[key] 

    # ê²°ê³¼ê°’ ì „ë‹¬
    next_node_list = [ {'node_name': 'response', 'node_type': 'responseNode'} ]
        
    return return_next_node( node_name, next_node_list, return_value ) 
""" + "\n" + "\n" + "\n"

    
    # create function 
    for node in create_node_json['nodes'] : 
        node_name = node['data']['label']
        if node['type'] == 'startNode': 
            python_code += start_node_code.replace( "**startNode**", node_name ) 
            
        elif node['type'] == 'promptNode': 
            python_code += prompt_node_code.replace( "**promptNode**", node_name ) 
            
        elif node['type'] == 'mergeNode': 
            python_code += merge_node_code.replace( "**mergeNode**", node_name ) 
    
        elif node['type'] == 'endNode': 
            python_code += end_node_code.replace( "**endNode**", node_name ) 

        elif node['type'] == 'functionNode': 
            py_code   = node['data']['code']
            parsed = ast.parse(py_code)
            func_def = next((node for node in parsed.body if isinstance(node, ast.FunctionDef)), None)
            function_name = func_def.name            
            python_code += create_function_node(node_name, py_code, function_name) + "\n" + "\n"
        


    
    # create node  
    python_code += "\ngraph = StateGraph(MyState)\n" 
    tmp_create_node = """graph.add_node("_**tmp_node**", node_**tmp_node**)\n"""
    
    for node in create_node_json['nodes'] : 
        node_name = node['data']['label']
        if node['type'] == 'startNode': 
            python_code += tmp_create_node.replace( "**tmp_node**", node_name ) 
            
        elif node['type'] == 'promptNode': 
            python_code += tmp_create_node.replace( "**tmp_node**", node_name ) 
            
        elif node['type'] == 'mergeNode': 
            tmp_create_merge_node = """graph.add_node("_**tmp_node**", node_**tmp_node**, defer=True)\n"""
            python_code += tmp_create_merge_node.replace( "**tmp_node**", node_name ) 
    
        elif node['type'] == 'endNode': 
            python_code += tmp_create_node.replace( "**tmp_node**", node_name ) 

        elif node['type'] == 'functionNode': 
            python_code += tmp_create_node.replace( "**tmp_node**", node_name ) 


    python_code += "\n"
    
    cnt = 0 
    for node in create_node_json['edges'] : 
        source_node_id = node['source']
        target_node_id = node['target']
        source_node_name = node_id_to_node_label[source_node_id]['node_name'] 
        target_node_name = node_id_to_node_label[target_node_id]['node_name'] 
        if node_id_to_node_label[source_node_id]['node_type'] == 'startNode' : 
            if cnt == 0 : 
                python_code += """graph.add_edge(START, "_**tmp_node**")\n""".replace( "**tmp_node**", source_node_name) 
                cnt = 1
            python_code += """graph.add_edge("_**source**", "_**target**")\n""".replace( "**source**", source_node_name ).replace( "**target**", target_node_name) 
            
        else : 
            python_code += """graph.add_edge("_**source**", "_**target**")\n""".replace( "**source**", source_node_name ).replace( "**target**", target_node_name) 
    
    
    for node in create_node_json['edges'] : 
        source_node_id = node['source']
        target_node_id = node['target']
        source_node_name = node_id_to_node_label[source_node_id]['node_name'] 
        target_node_name = node_id_to_node_label[target_node_id]['node_name'] 
       
        if node_id_to_node_label[target_node_id]['node_type'] == 'endNode' : 
            python_code += """graph.add_edge("_**source**", END)\n""".replace( "**source**", target_node_name )
            python_code += """app = graph.compile()"""
            break 
    


    return python_code 
    

    

    




@app.post('/workflow/export/python/langgraph')
def react_to_python(msg: dict = Body(...)):
    return {"langgraph" : react_to_python_langgraph(msg)}


